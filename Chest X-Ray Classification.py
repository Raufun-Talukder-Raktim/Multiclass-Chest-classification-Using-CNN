# -*- coding: utf-8 -*-
"""190104151_softCom_Assignment_01ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t_Xrg_23LDFg5yHi-EBCI2NjXzmX9PJQ
"""
 
from google.colab import drive
drive.mount('/content/drive')

dataset = !unzip "/content/drive/MyDrive/Softcom/archive_2.zip"

!mkdir dataset_all_2
folder1 = '/content/COVID'
folder2 = '/content/NORMAL'
folder3 = '/content/PNEUMONIA'

!mv '/content/COVID' '/content/dataset_all_2'
!mv '/content/NORMAL' '/content/dataset_all_2'
!mv '/content/PNEUMONIA' '/content/dataset_all_2'

!ls /content/dataset_all_2

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import shutil
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as dsets
import tensorflow as tf
from tensorflow.keras import layers, models

import os
import numpy as np
import cv2
from sklearn.model_selection import train_test_split


data_dir = '/content/dataset_all_2'


classes = [ 'PNEUMONIA', 'NORMAL','COVID']


data = []
labels = []


for class_name in classes:
    class_dir = os.path.join(data_dir, class_name)
    for image_name in os.listdir(class_dir):
        image_path = os.path.join(class_dir, image_name)
        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)
        resized_image = cv2.resize(image, (256, 256))
        data.append(resized_image)
        labels.append(classes.index(class_name))

data = np.array(data, dtype=np.float32)/255.0
labels = np.array(labels)


X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.2, random_state=50, stratify=labels)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=50,stratify=y_temp)

print("Train set size:", X_train.shape[0])
print("Validation set size:", X_val.shape[0])
print("Test set size:", X_test.shape[0])


train_dataloader = list(zip(X_train, y_train))
val_dataloader = list(zip(X_val, y_val))
test_dataloader = list(zip(X_test, y_test))

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, images, labels, transform=None):
        self.images = images
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, index):
        image = self.images[index]
        label = self.labels[index]

        if self.transform:
            image = self.transform(image)

        return image, label

import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.ToTensor(),
])


train_dataset = CustomDataset(X_train, y_train, transform=transform)
train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)

val_dataset = CustomDataset(X_val, y_val, transform=transform)
val_dataloader =torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)

test_dataset = CustomDataset(X_test, y_test, transform=transform)
test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)

torch.manual_seed(50)

class CNNClassifier(nn.Module):
    def __init__(self, num_classes):
        super(CNNClassifier, self).__init__()
        self.cnn_layer_1 = nn.Conv2d(in_channels=1, out_channels=16,kernel_size=3, stride=1, padding=1)
        self.cnn_layer_2 = nn.Conv2d(in_channels=16, out_channels=32,kernel_size=3, stride=1, padding=1)
        self.cnn_layer_3 = nn.Conv2d(in_channels=32, out_channels=48,kernel_size=3, stride=1, padding=1)

        self.flatten = nn.Flatten()
        self.maxpool = nn.MaxPool2d(2,2)

        self.linear_layer_1 = nn.Linear(32*64*64, 512)
        self.linear_layer_2 = nn.Linear(512, 128)
        self.linear_layer_3 = nn.Linear(128, 10)
        self.linear_layer_4 = nn.Linear(10, 3)

        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.dropout = nn.Dropout(.2)

        # self.flatten = nn.Flatten()

    def forward(self, x):

        x = self.cnn_layer_1(x)
        x = self.dropout(x)
        x = self.relu(x)
        x = self.maxpool(x)

        #print(x.shape)

        x = self.cnn_layer_2(x)
        x = self.dropout(x)
        x = self.relu(x)
        x = self.maxpool(x)


        #print(x.shape)

        x = self.flatten(x)
        #print(x.shape)

        x = self.linear_layer_1(x)
        x = self.dropout(x)
        x = self.relu(x)

        x = self.linear_layer_2(x)
        x = self.dropout(x)
        x = self.relu(x)

        x = self.linear_layer_3(x)
        x = self.dropout(x)
        x = self.relu(x)

        x = self.linear_layer_4(x)
        #logits = self.sigmoid(x)
        return x

num_classes = len(classes)

cnn_model = CNNClassifier(num_classes=num_classes)

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
# optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)
optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)


# Function to train the model
def train_model(model, dataloader, criterion, optimizer, num_epochs):
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        for images, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
        epoch_loss = running_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}")
# Train the model
num_epochs = 10
train_model(cnn_model, train_dataloader, criterion, optimizer, num_epochs)



# Function to evaluate the model
def evaluate_model(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in dataloader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    accuracy = 100 * correct / total
    print(f"Accuracy: {accuracy:.4f}%")


print("Validation set accuracy:")
evaluate_model(cnn_model, val_dataloader)


print("Test set accuracy:")
evaluate_model(cnn_model, test_dataloader)
